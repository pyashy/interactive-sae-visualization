{"text_id":125872,"tokens":["The"," authors"," present"," an"," interesting"," approach"," to"," learn"," histogram"," filters"," for"," image"," processing",".","\n","Their"," method"," is"," based"," on"," learning"," from"," examples"," using"," gradient"," descent"," with"," momentum"," term"," in"," order"," to"," speed"," up"," convergence"," of"," training"," process","."," ","\n","They"," show"," that"," their"," algorithm"," can"," be"," used"," as"," end","-","to","-","end"," train","able"," filter"," which"," does"," not"," require"," any"," preprocessing"," steps"," such"," as"," feature"," extraction","."," ","\n"," ","\n"," The"," main"," contribution"," lies"," in"," introducing"," new"," loss"," function"," called"," \"","cross","-","entropy","\""," between"," output"," distribution"," (","histogram",")"," and"," target"," distribution"," instead"," of"," MSE"," error"," commonly"," used"," in"," this"," field",".","  ","\n"," ","\n"," ","\n"," ","\n"," Strengths",":"," ","\n","1","."," End","-","to","-","end"," lear","nable"," without"," requiring"," pre","-","processing"," step"," like"," extracting"," features"," ","\n"," ","\n"," ","2","."," New"," cross","-","entropy"," loss"," function"," introduced"," ","\n"," ","\n"," Weak","nesses",":"," ","\n","1","."," No"," comparison"," made"," against"," other"," methods"],"sub_source":"peerread","model":"bloomz","label":1}