{"text_id":8530,"tokens":["This"," paper"," proposes"," a"," sequence"," transduction"," model"," that"," first"," uses"," a"," traditional"," statistical"," alignment"," methods"," to"," provide"," alignments"," for"," an"," encoder","-","decoder"," type"," model","."," The"," paper"," provides"," experiments"," on"," a"," number"," of"," morphological"," inflection"," generation"," datasets","."," They"," shows"," an"," improvement"," over"," other"," models",","," although"," they"," have"," much"," smaller"," improvements"," over"," a"," soft"," attention"," model"," on"," some"," of"," their"," tasks","."," ","\n","I"," found"," this"," paper"," to"," be"," well","-","written"," and"," to"," have"," very"," thorough"," experiments","/","analysis",","," but"," I"," have"," concerns"," that"," this"," work"," isn","'","t"," particularly"," different"," from"," previous"," approaches"," and"," thus"," has"," a"," more"," focused"," contribution"," that"," is"," limited"," to"," its"," application"," on"," this"," type"," of"," shorter"," input"," (","the"," authors"," \"","suggest","\""," that"," their"," approach"," is"," sufficient"," for"," shorter"," sequences",","," but"," don","'","t"," compare"," against"," the"," approach"," of"," Chor","owski"," et"," al","."," ","2","0","1","5"," or"," Jail","ty"," el"," at"," ","2","0","1","6",").","\n","In"," summary",","," I"," found"," this"," paper"," to"," be"," well","-","executed","/","well","-","written",","," but"," it","'","s"," novelty"," and"," scope"," too"," small","."," That"," said",","," I"," feel"," this"," work"," would"," make"," a"," very"," good"," short"," paper","."],"sub_source":"peerread","model":"human","label":0}