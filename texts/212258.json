{"text_id":212258,"tokens":["We"," present"," five"," variants"," of"," the"," standard"," Long"," Short","-","term"," Memory"," (","LSTM",")"," recurrent"," neural"," networks"," by"," uniformly"," reducing"," blocks"," of"," adaptive"," parameters"," in"," the"," g","ating"," mechanisms","."," For"," simplicity",","," we"," refer"," to"," these"," variants"," as"," emp","h","{","compressed","}"," LSTM","."," We"," apply"," this"," compression"," technique"," to"," six"," different"," datasets",":"," a"," character","-","level"," language"," modeling"," task",","," a"," next","-","word"," prediction"," task"," on"," three"," different"," corpora"," (","En","ron",","," Penn"," Tree","bank"," and"," One"," Billion"," Words","),"," a"," multilingual"," character","-","level"," language"," modeling"," task",","," and"," the"," Google"," Speech"," Commands"," dataset","."," Our"," experiments"," show"," that"," the"," original"," LSTM"," model"," is"," over","parameter","ized"," with"," respect"," to"," the"," training"," data"," available",","," and"," the"," compressed"," variants"," achieve"," comparable"," or"," better"," performance","."," This"," result"," is"," especially"," surprising"," for"," the"," multilingual"," and"," Google"," Speech"," Commands"," datasets",","," which"," contain"," a"," much"," smaller"," amount"," of"," training"," data","."],"sub_source":"sci_gen","model":"30B","label":1}