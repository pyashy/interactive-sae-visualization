{"text_id":182575,"tokens":["Lex","NLP"," is"," an"," open"," source"," Python"," package"," focused"," on"," natural"," language"," processing"," and"," machine"," learning"," for"," legal"," and"," regulatory"," text","."," The"," package"," includes"," functionality"," to"," (","i",")"," segment"," documents",","," (","ii",")"," identify"," sentences",","," (","iii",")"," tokenize"," words",","," (","iv",")"," part","-","of","-","speech"," tag"," words",","," (","v",")"," extract"," n","-","grams",","," (","vi",")"," extract"," citation"," references",","," (","vii",")"," identify"," part","-","of","-","speech"," patterns",","," (","viii",")"," convert"," dates"," into"," a"," standard"," format",","," (","ix",")"," identify"," entities",","," (","x",")"," identify"," entity"," categories",","," (","xi",")"," link"," entities"," to"," unique"," identifiers",","," (","xii",")"," cluster"," entities",","," (","xiii",")"," create"," networks"," of"," entities"," and"," their"," relationships",","," and"," (","xiv",")"," perform"," sentiment"," analysis","."," Lex","NLP"," is"," designed"," to"," work"," with"," text"," in"," the"," English"," language","."," The"," package"," is"," released"," under"," the"," MIT"," License","."," Installation"," To"," install"," Lex","NLP",","," simply"," use"," pip",":"," pip"," install"," lex","nlp"," Alternatively",","," you"," can"," clone"," the"," GitHub"," repository"," and"," install"," Lex","NLP"," manually",":"," git"," clone"," cd"," lex","predict","-","lex","nlp"," python"," setup",".","py"," install"," Usage"," The"," following"," example"," shows"," how"," to"," use"," Lex","NLP"," to"," segment"," a"," document",","," identify"," sentences",","," and"," tokenize"," words",":"," import"," lex","nlp",".","extract",".","en",".","segments",".","sentences"," from"," lex","nlp",".","extract",".","en",".","tokens"," import"," get","token","list"," text"," \""," '"," Lex","NLP"," is"," an"," open"," source"," Python"," package"," focused"," on"," natural"," language"," processing"," and"," machine"," learning"," for"," legal"," and"," regulatory"," text","."," The"," package"," includes"," functionality"," to"," (","i",")"," segment"," documents",","," (","ii",")"," identify"," sentences",","," (","iii",")"," tokenize"," words",","," (","iv",")"," part","-","of","-","speech"," tag"," words",","," (","v",")"," extract"," n","-","grams",","," (","vi",")"," extract"," citation"," references",","," (","vii",")"," identify"," part","-","of","-","speech"," patterns",","," (","viii",")"," convert"," dates"," into"," a"," standard"," format",","," (","ix",")"," identify"," entities",","," (","x",")"," identify"," entity"," categories",","," (","xi",")"," link"," entities"," to"," unique"," identifiers",","," (","xii",")"," cluster"," entities",","," (","xiii",")"," create"," networks"," of"," entities"," and"," their"," relationships",","," and"," (","xiv",")"," perform"," sentiment"," analysis","."," \""," '"," sentence","list"," list"," (","lex","nlp",".","extract",".","en",".","segments",".","sentences",".","get","sentence","list"," (","text"," print"," Sentences",":"," for"," sentence"," in"," sentence","list",":"," print"," (","sentence",")"," print"," print"," Tokens",":"," for"," sentence"," in"," sentence","list",":"," token","list"," get","token","list"," (","sentence",")"," print"," :"," .","format"," (","sentence",","," token","list"," The"," above"," code"," produces"," the"," following"," output",":"," Sentences",":"," Lex","NLP"," is"," an"," open"," source"," Python"," package"," focused"," on"," natural"," language"," processing"," and"," machine"," learning"," for"," legal"," and"," regulatory"," text","."," The"," package"," includes"," functionality"," to"," (","i",")"," segment"," documents",","," (","ii",")"," identify"," sentences",","," (","iii",")"," tokenize"," words",","," (","iv",")"," part","-","of","-","speech"," tag"," words",","," (","v",")"," extract"," n","-","grams",","," (","vi",")"," extract"," citation"," references",","," (","vii",")"," identify"," part","-","of","-","speech"," patterns",","," (","viii",")"," convert"," dates"," into"," a"," standard"," format",","," (","ix",")"," identify"," entities",","," (","x",")"," identify"," entity"," categories",","," (","xi",")"," link"," entities"," to"," unique"," identifiers",","," (","xii",")"," cluster"," entities",","," (","xiii",")"," create"," networks"," of"," entities"," and"," their"," relationships",","," and"," (","xiv",")"," perform"," sentiment"," analysis","."," Tokens",":"," Lex","NLP"," is"," an"," open"," source"," Python"," package"," focused"," on"," natural"," language"," processing"," and"," machine"," learning"," for"," legal"," and"," regulatory"," text",".:"," Lex","NLP","',"," '","is","',"," '","an","',"," '","open","',"," '","source","',"," '","Python","',"," '","package","',"," '","focused","',"," '","on","',"," '","natural","',"," '","language","',"," '","processing","',"," '","and","',"," '","machine","',"," '","learning","',"," '","for","',"," '","legal","',"," '","and","',"," '","regulatory","',"," '","text","',"," '."," The"," package"," includes"," functionality"," to"," (","i",")"," segment"," documents",","," (","ii",")"," identify"," sentences",","," (","iii",")"," tokenize"," words",","," (","iv",")"," part","-","of","-","speech"," tag"," words",","," (","v",")"," extract"," n","-","grams",","," (","vi",")"," extract"," citation"," references",","," (","vii",")"," identify"," part","-","of","-","speech"," patterns",","," (","viii",")"," convert"," dates"," into"," a"," standard"," format",","," (","ix",")"," identify"," entities",","," (","x",")"," identify"," entity"," categories",","," (","xi",")"," link"," entities"," to"," unique"," identifiers",","," (","xii",")"," cluster"," entities",","," (","xiii",")"," create"," networks"," of"," entities"," and"," their"," relationships",","," and"," (","xiv",")"," perform"," sentiment"," analysis",".:"," The","',"," '","package","',"," '","includes","',"," '","function","ality","',"," '","to","',"," '"," (","i",")"," ',"," '","segment","',"," '","documents","',"," ',',"," '"," (","ii",")"," ',"," '","identify","',"," '","sentences","',"," ',',"," '"," (","iii",")"," ',"," '","tokenize","',"," '","words","',"," ',',"," '"," (","iv",")"," ',"," '","part","-","of","-","speech","',"," '","tag","',"," '","words","',"," ',',"," '"," (","v",")"," ',"," '","extract","',"," '","n","-","grams","',"," ',',"," '"," (","vi",")"," ',"," '","extract","',"," '","citation","',"," '","references","',"," ',',"," '"," (","vii",")"," ',"," '","identify","',"," '","part","-","of","-","speech","',"," '","pattern"],"sub_source":"sci_gen","model":"text-davinci-002","label":1}